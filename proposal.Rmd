---
title: Stat 502 Project Proposal
author: Arman Bilge, Cheng Wang, and Zexuan Zhou
date: November 2, 2016
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = T)
options(digits = 3)
df <- log(data.frame(t(apply(read.table('proposal-results.txt', header = T), 1, function(x) x / x[1]))))
```

## Background

Python is one of the most popular programming languages, with many applications in scientific computing.
Thus, maximizing the performance of Python code is of particular interest.
Because Python is an interpreted language, its performance depends on the particular runtime used.
Interpreted languages often rely on a just-in-time (JIT) compiler to dynamically optimize the code at runtime, which the official CPython^[https://www.python.org/] interpreter lacks.
Fortunately, there are many options for Python runtimes besides CPython, including PyPy,^[http://pypy.org/] Jython,^[http://www.jython.org/] and IronPython.^[http://ironpython.net/]
PyPy uses its own JIT compiler, while Jython and IronPython are built on top of the Java and .NET frameworks, respectively, which encompass their own JIT compilers.
In this study we hope to determine whether the use of a runtime with a JIT compiler offers a performance benefit over the official CPython implementations.

## Pilot study

Each experimental unit is a small program that is a solution to one of the Project Euler^[https://projecteuler.net/] problems.
In this pilot study we considered 50 such programs.
Each program was run once in each of the CPython (versions 2 and 3), PyPy, Jython, and IronPython interpreters on a personal laptop running macOS 10.9.
The program run time was measured using the `time` module in the Python standard library from just before to just after code execution.
Note that this purposefully excludes the time that the interpreters took to start up or shut down.

```{r, dev='pdf', fig.cap=''}
boxplot(df)
```

```{r}
anova(lm(values ~ ind, data=stack(df)))
```

## Proposal for larger experiment

We will implement some additional strategies to ensure robustness of our results, starting with performing multiple replicates for each experimental unit.
As it is challenging to completely control for the state of the computer that we are running the tests on, we expect there to be some noise in our measurements.
Performing multiple replicate runs for each program should help reduce the effects of this noise on our results.
Furthermore, it is not straightforward to accurately measure the performance of a runtime using a JIT compiler.
A method may need to be called hundreds or thousands of times before the JIT compiler determines that it is worth optimizing.
Because we want to assess the asymptotic performance of the interpreters, we will do several "warm-up" runs of the program we are testing before taking any measurements to provide ample opportunity for the JIT compiler to make its optimizations.

### Power calculation
