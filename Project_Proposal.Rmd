---
title: "Stat 502 Project Proposal"
author: "Arman Bilge, Cheng Wang, and Zexuan Zhou"
date: "November 2, 2016"
output: pdf_document
---

\frenchspacing
```{r setup, echo=FALSE}
setwd("~/STAT 502/Project")
knitr::opts_chunk$set(cache = T)
options(digits = 3)
df <- log(data.frame(t(apply(read.table('proposal-results_new.txt', header = T), 1, function(x) x / x[1]))))
```
## Background
Python is one of the most popular programming languages, with many applications in scientific computing.
Thus, maximizing the performance of Python code is of particular interest.
Because Python is an interpreted language, its performance depends on the particular runtime used.
Interpreted languages often rely on a just-in-time (JIT) compiler to dynamically optimize the code at runtime, which the official CPython^[https://www.python.org/] interpreter lacks.
Fortunately, there are many options for Python runtimes besides CPython, including PyPy,^[http://pypy.org/] Jython,^[http://www.jython.org/] and IronPython.^[http://ironpython.net/]
PyPy uses its own JIT compiler, while Jython and IronPython are built on top of the Java and .NET frameworks, respectively, which encompass their own JIT compilers.
In this study we hope to determine whether the use of a runtime with a JIT compiler offers a performance benefit over the official CPython implementations.

## Pilot study
Each experimental unit is a small program that is a solution to one of the Project Euler^[https://projecteuler.net/] problems.
In this pilot study we considered 47 such programs.
Each program was run once in each of the CPython (versions 2 and 3), PyPy, Jython, and IronPython interpreters on a personal laptop running macOS 10.9.
The program run time was measured using the `time` module in the Python standard library from just before to just after code execution.
Note that this purposefully excludes the time that the interpreters took to start up or shut down.
```{r, dev='pdf', fig.cap='', echo=FALSE}
boxplot(df[,2:5])
```

```{r, echo=FALSE}
df.asrow<-c(df[,2],df[,3],df[,4],df[,5])
df.py3<-df[,2]
df.pypy<-df[,3]
df.jpy<-df[,4]
df.ipy<-df[,5]
df.desc<-matrix(c(mean(df.py3),mean(df.pypy),mean(df.jpy),mean(df.ipy),mean(df.asrow),sd(df.py3),sd(df.pypy),sd(df.jpy),sd(df.ipy),sd(df.asrow)),ncol=2,byrow=FALSE)
rownames(df.desc)<-c("Python3", "PyPy", "Jython", "IronPy", "Sample")
colnames(df.desc)<-c("Mean", "SD")
```

```{r, echo=FALSE}
df.desc
```


We would like to do an one-way ANOVA test with the following hypothesis:
$$H_0: \mu_1=\mu_2=\mu_3=\mu_4 \  \text{vs} \  \ H_a: \text{At least one differs from the others.}$$
Where $\mu_1, \mu_2, \mu_3, \mu_4$ correpsonds the true population average compiling time ratio of Python3 to Python, PyPy to Python, Jython to Python, and IronPython to Python, respectively.
```{r, echo=FALSE}
anova(lm(values ~ ind, data=stack(df)))
```
The ANOVA test gives a p-value of 1.8e-10 << 0.05, with degrees of freedom (4, 230). Therefore we reject the null and conclude that one of the compilers have different compiling time ratio.

## Proposal for larger experiment
We will implement some additional strategies to ensure robustness of our results, starting with performing multiple replicates for each experimental unit.
As it is challenging to completely control for the state of the computer that we are running the tests on, we expect there to be some noise in our measurements.
Performing multiple replicate runs for each program should help reduce the effects of this noise on our results.
Furthermore, it is not straightforward to accurately measure the performance of a runtime using a JIT compiler.
A method may need to be called hundreds or thousands of times before the JIT compiler determines that it is worth optimizing.
Because we want to assess the asymptotic performance of the interpreters, we will do several "warm-up" runs of the program we are testing before taking any measurements to provide ample opportunity for the JIT compiler to make its optimizations.

## Power calculation
We use $MSE$ as our estimator for $\sigma^2$ and $\alpha = 0.05$ as our test level. 
```{r, echo=FALSE}
n1<-n2<-n3<-n4<-47
m<-4; N<-length(df.asrow)
tau1<-mean(df.py3)-mean(df.asrow)
tau2<-mean(df.pypy)-mean(df.asrow)
tau3<-mean(df.jpy)-mean(df.asrow)
tau4<-mean(df.ipy)-mean(df.asrow)
tausum<-(tau1^2+tau2^2+tau3^2+tau4^2); 
MSE<-1/m*(var(df.py3)+var(df.pypy)+var(df.jpy)+var(df.ipy))
alpha<-0.05 ; n<-seq(5,50)
f.lambda<-n*tausum/MSE
f.crit<-qf(1-alpha/2,m-1,n*m-m)
f.power<- 1-pf(f.crit,m-1,N-m,ncp=f.lambda)
```
```{r, echo=FALSE}
attach(mtcars)
par(mfrow=c(1,3))
plot(x=n,y=f.crit,ylab="F.crit")
plot(x=n,y=f.lambda,ylab=expression(lambda),main=expression(paste("m = 4     ",alpha," = 0.05")))
plot(x=n,y=f.power,ylab="power")
```
From the plots we see that if we want to ensure a high power, we need the sample size of each group to be larger than 30.
